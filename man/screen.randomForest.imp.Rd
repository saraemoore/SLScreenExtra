% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/rf.R
\name{screen.randomForest.imp}
\alias{screen.randomForest.imp}
\title{"Best of both worlds" Random Forest screening algorithm}
\usage{
screen.randomForest.imp(Y, X, family, obsWeights, id,
  selector = c("cutoff.biggest.diff", "cutoff.k", "cutoff.k.percent"),
  k = switch(selector, cutoff.k = ceiling(0.5 * ncol(X)), cutoff.k.percent =
  0.5, NULL), nTree = 1000, mTry = ifelse(family$family == "gaussian",
  floor(sqrt(ncol(X))), max(floor(ncol(X)/3), 1)),
  nodeSize = ifelse(family$family == "gaussian", 5, 1),
  importanceType = c("permutation", "impurity"), maxNodes = NULL,
  verbose = FALSE, ...)
}
\arguments{
\item{Y}{Outcome (numeric vector). See \code{\link[SuperLearner]{SuperLearner}}
for specifics.}

\item{X}{Predictor variable(s) (data.frame or matrix). See
\code{\link[SuperLearner]{SuperLearner}} for specifics.}

\item{family}{Error distribution to be used in the model:
\code{\link[stats]{gaussian}} or \code{\link[stats]{binomial}}.
Currently unused. See \code{\link[SuperLearner]{SuperLearner}}
for specifics.}

\item{obsWeights}{Optional numeric vector of observation weights. Currently
unused.}

\item{id}{Cluster identification variable. Currently unused.}

\item{selector}{A string corresponding to a subset selecting function
implemented in the FSelector package. One of:
\code{\link[FSelector]{cutoff.biggest.diff}} (default),
\code{\link[FSelector]{cutoff.k}}, or
\code{\link[FSelector]{cutoff.k.percent}}.}

\item{k}{Passed through to the \code{selector} in the case where \code{selector} is
\code{\link[FSelector]{cutoff.k}} or \code{\link[FSelector]{cutoff.k.percent}}.
Otherwise, should remain NULL (the default). For \code{\link[FSelector]{cutoff.k}},
this is an integer indicating the number of features to keep from \code{X}.
For \code{\link[FSelector]{cutoff.k.percent}}, this is instead the proportion
of features to keep.}

\item{nTree}{Integer. Number of trees. Default: 1000.}

\item{mTry}{Integer. Number of columns of \code{X} sampled at each split.
Default: square root (\code{gaussian()} family) or one third
(\code{binomial()} family) of total number of features, rounded down.}

\item{nodeSize}{Integer. Minimum number of observations in terminal nodes.
Default: 5 (\code{gaussian()} family) or 1 (\code{binomial()} family).}

\item{importanceType}{Importance type. \code{"permutation"} (default) indicates
mean decrease in accuracy (for \code{binomial()} family) or percent increase
in mean squared error (for \code{gaussian()} family) when comparing
predictions using the original variable versus a permuted version of the
variable (column of \code{X}). \code{"impurity"} indicates increase in
node purity achieved by splitting on that column of \code{X} (for
\code{binomial()} family, measured by Gini index; for \code{gaussian()},
measured by residual sum of squares). See
\code{\link[randomForest]{randomForest}} for more details, where
\code{"permutation"} corresponds to \code{type = 1} and \code{"impurity"}
corresponds to \code{type = 2}.}

\item{maxNodes}{Maximum number of terminal nodes allowed in a tree. Default
(\code{NULL}) indicates that trees should be grown to maximum possible size.
See \code{\link[randomForest]{randomForest}} for more details.}

\item{verbose}{Should debugging messages be printed? Default: \code{FALSE}.}

\item{...}{Currently unused.}
}
\value{
A logical vector with length equal to \code{ncol(X)}.
}
\description{
Customizability of \code{\link[SuperLearner]{screen.randomForest}} combined
with the \code{\link[FSelector]{cutoff}} selectors of
\code{\link[FSelector]{FSelector}}.
}
\examples{
data(iris)
Y <- as.numeric(iris$Species=="setosa")
X <- iris[,-which(colnames(iris)=="Species")]
screen.randomForest.imp(Y, X, binomial(), selector = "cutoff.k.percent", k = 0.75)

data(mtcars)
Y <- mtcars$mpg
X <- mtcars[,-which(colnames(mtcars)=="mpg")]
screen.randomForest.imp(Y, X, gaussian(), importanceType = "impurity")

# based on examples in SuperLearner package
set.seed(1)
n <- 100
p <- 20
X <- matrix(rnorm(n*p), nrow = n, ncol = p)
X <- data.frame(X)
Y <- X[, 1] + sqrt(abs(X[, 2] * X[, 3])) + X[, 2] - X[, 3] + rnorm(n)

library(SuperLearner)
sl = SuperLearner(Y, X, family = gaussian(), cvControl = list(V = 2),
                  SL.library = list(c("SL.glm", "All"),
                                    c("SL.glm", "screen.randomForest.imp")))
sl
sl$whichScreen
}
