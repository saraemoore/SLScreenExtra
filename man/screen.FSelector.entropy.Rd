% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/fselector.R
\name{screen.FSelector.entropy}
\alias{screen.FSelector.entropy}
\title{Entropy-based screening algorithms}
\usage{
screen.FSelector.entropy(Y, X, family, filter = c("symmetrical.uncertainty",
  "gain.ratio", "information.gain"), unit = formals(information.gain)$unit,
  selector = c("cutoff.biggest.diff", "cutoff.k", "cutoff.k.percent"),
  k = switch(selector, cutoff.k = ceiling(0.5 * ncol(X)), cutoff.k.percent =
  0.5, NULL), verbose = FALSE, ...)
}
\arguments{
\item{Y}{Outcome (numeric vector). See \code{\link[SuperLearner]{SuperLearner}}
for specifics.}

\item{X}{Predictor variable(s) (data.frame or matrix). See
\code{\link[SuperLearner]{SuperLearner}} for specifics.}

\item{family}{Error distribution to be used in the model:
\code{\link[stats]{gaussian}} or \code{\link[stats]{binomial}}.
Currently unused. See \code{\link[SuperLearner]{SuperLearner}}
for specifics.}

\item{filter}{Character string. One of: \code{"symmetrical.uncertainty"} (default),
\code{"gain.ratio"}, or \code{"information.gain"}}

\item{unit}{Unit in which entropy is measured by
\code{\link[entropy]{entropy}}. Character string. One of: \code{"log"}
(default), \code{"log2"}, or \code{"log10"}.}

\item{selector}{A string corresponding to a subset selecting function
implemented in the FSelector package. One of:
\code{\link[FSelector]{cutoff.biggest.diff}},
\code{\link[FSelector]{cutoff.k}}, \code{\link[FSelector]{cutoff.k.percent}},
or \code{"all"}. Note that \code{"all"} is a not a function but indicates
pass-thru should be performed in the case of a \code{filter} which selects
rather than ranks features. Default: \code{"cutoff.biggest.diff"}.}

\item{k}{Passed through to the \code{selector} in the case where \code{selector} is
\code{\link[FSelector]{cutoff.k}} or \code{\link[FSelector]{cutoff.k.percent}}.
Otherwise, should remain NULL (the default). For \code{\link[FSelector]{cutoff.k}},
this is an integer indicating the number of features to keep from \code{X}.
For \code{\link[FSelector]{cutoff.k.percent}}, this is instead the proportion
of features to keep.}

\item{verbose}{Should debugging messages be printed? Default: \code{FALSE}.}

\item{...}{Currently unused.}
}
\value{
A logical vector with length equal to \code{ncol(X)}.
}
\description{
Information gain, gain ratio, and symmetrical uncertainty scores are
calculated from the Shannon entropy of \code{X} and \code{Y}. Information
gain (\code{\link[FSelector]{information.gain}}) (or, equivalently, mutual
information) is a measure of entropy reduction achieved by the feature with
regard to the outcome, \code{Y}. The information gain ratio
(\code{\link[FSelector]{gain.ratio}}) is a normalized version of information
gain, normalized by the entropy of the feature. Symmetrical uncertainty
(\code{\link[FSelector]{symmetrical.uncertainty}}) is a normalized and
bias-corrected version of information gain. Implemented for \code{binomial()}
family only and designed to be used with binary or categorical \code{X}.
Continuous \code{X} will be discretized by \code{\link{FSelector}} and
\code{\link[RWeka]{Discretize}} using the MDL method (Fayyad & Irani, 1993).
}
\examples{
data(iris)
Y <- as.numeric(iris$Species=="setosa")
X <- iris[,-which(colnames(iris)=="Species")]
screen.FSelector.entropy(Y, X, binomial(), selector = "cutoff.k.percent", k = 0.75)

# based on example in SuperLearner package
set.seed(1)
n <- 100
p <- 20
X <- matrix(rnorm(n*p), nrow = n, ncol = p)
X <- data.frame(X)
Y <- rbinom(n, 1, plogis(.2*X[, 1] + .1*X[, 2] - .2*X[, 3] + .1*X[, 3]*X[, 4] - .2*abs(X[, 4])))

library(SuperLearner)
sl = SuperLearner(Y, X, family = binomial(), cvControl = list(V = 2),
                  SL.library = list(c("SL.lm", "All"),
                                    c("SL.lm", "screen.FSelector.entropy")))
sl
sl$whichScreen
}
\references{
\url{http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.37.4643}
\url{http://hdl.handle.net/2014/35171}
}
